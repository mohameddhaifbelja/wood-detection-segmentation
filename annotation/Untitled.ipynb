{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0cd9d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# some handy functions to use along widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "# widget packages\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%matplotlib inline\n",
    "import IPython.display as display\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24ccdd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class configuration:\n",
    "    def __init__(self):\n",
    "        self.cls = -1\n",
    "        \n",
    "    @property\n",
    "    def cls(self):\n",
    "        return self.cls\n",
    "    @cls.setter\n",
    "    def cls(self, newcls):\n",
    "        self.cls = newcls\n",
    "        if cls in [0,1]:\n",
    "            img_path = os.path.join(self.output_folder, f'{self.img_name}-{self.i}_{self.cls}.png')\n",
    "            cv2.imwrite(self.img_path,self.arr)\n",
    "        if cls == 2:\n",
    "            small_arrs = np.array_split(img, img.shape[1]//5, axis=1)\n",
    "            \n",
    "            for j, small_arr in enumerate(small_arrs):\n",
    "                cls = -1\n",
    "                cv2.imshow(\"small_img\",small_arr)\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()\n",
    "                while int(cls) not in [0,1,2]:\n",
    "                    display.display(buttons_classification, clear=True)\n",
    "                    time.sleep(2)\n",
    "                img_path = os.path.join(output_folder,f'{img_name}-{i}-{j}_{cls}.png')\n",
    "                cv2.imwrite(output_folder,small_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca5673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.cls = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {'lug':1, 'wood':0, 'ambiguous':2}\n",
    "skip_enum = {'proceed':1, 'skip':2}\n",
    "cls = -1\n",
    "cnt = -1\n",
    "output = widgets.Output()\n",
    "\n",
    "@output.capture(clear_output=False,wait=True) # based on https://github.com/jupyter-widgets/ipywidgets/issues/1846 and https://ipywidgets.readthedocs.io/en/latest/examples/Output%20Widget.html\n",
    "def classification(b):\n",
    "    global cls \n",
    "    cls = class_dict[b.description]\n",
    "\n",
    "def skip_fn(b):\n",
    "    cnt = skip_enum[b.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_change(button1, button2, button3): #<------ Rename to widget1, and add widget2\n",
    "    future = asyncio.Future()\n",
    "    def getvalue(change):\n",
    "        future.set_result(change.description)\n",
    "        button1.on_click(getvalue, remove=True) #<------ Rename to widget1\n",
    "        button2.on_click(getvalue, remove=True) #<------ New widget2\n",
    "        button3.on_click(getvalue, remove=True) #<------ New widget2\n",
    "\n",
    "        # we need to free up the binding to getvalue to avoid an IvalidState error\n",
    "        # buttons don't support unobserve\n",
    "        # so use `remove=True` \n",
    "    button1.on_click(getvalue) #<------ Rename to widget1\n",
    "    button2.on_click(getvalue) #<------ New widget2\n",
    "    button3.on_click(getvalue)\n",
    "    return future"
   ]
  },
  {
   "cell_type": "raw",
   "id": "079c2c07",
   "metadata": {},
   "source": [
    "\n",
    "list_to_tag = [\"one\", \"two\", \"three\", \"four\"]\n",
    "\n",
    "async def f():\n",
    "    for i in list_to_tag:\n",
    "        print('going to tag ', i)\n",
    "        x = await wait_for_change(button_lug, button_wood,button_ambiguious) #<---- Pass both buttons into the function\n",
    "        if x == \"wow\": #<--- use if statement to trigger different events for the two buttons\n",
    "            print(\"tagged \", i, \"with  %s\"%x)\n",
    "        else:\n",
    "            print(i, \"tagged with %s\"%x)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "\n",
    "asyncio.create_task(f())\n",
    "buttons_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63f46c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a577a4192d4a49a2e1272156aacc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='lug', style=ButtonStyle()), Button(description='wood', style=ButtonStyle())…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "button_lug = widgets.Button(description='lug', cls=1)\n",
    "button_wood = widgets.Button(description = \"wood\",cls=2)\n",
    "button_ambiguious = widgets.Button(description = \"ambiguous\", cls=3)\n",
    "\n",
    "buttons_classification = widgets.HBox([button_lug, button_wood, button_ambiguious])\n",
    "\n",
    "button_continue = widgets.Button(description =\"proceed\") \n",
    "button_skip = widgets.Button(description=\"skip\")\n",
    "\n",
    "buttons_first = widgets.HBox([button_continue, button_skip])\n",
    "\n",
    "button_lug.on_click(classification)\n",
    "button_ambiguious.on_click(classification)\n",
    "button_wood.on_click(classification)\n",
    "\n",
    "button_skip.on_click(skip_fn)\n",
    "button_continue.on_click(skip_fn)\n",
    "\n",
    "display.display(buttons_classification)\n",
    "print(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d400e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36270814",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../data/img_0.png\"\n",
    "img = cv2.imread(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04749c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arrs = np.array_split(img, img.shape[1]//5, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daaf67ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536, 5, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrs[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89503f59",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrs:\n\u001b[0;32m      2\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maaa\u001b[39m\u001b[38;5;124m\"\u001b[39m,arr)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for arr in arrs:\n",
    "    cv2.imshow(\"aaa\",arr)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26aba796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19fbaf62d40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(arrs[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8bb3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "def annotate_img(img, img_name, output_folder):\n",
    "    arrs = np.array_split(img, img.shape[1]//50, axis=1)\n",
    "    global cls\n",
    "    cls = -1\n",
    "    print(cls)\n",
    "    for i, arr in enumerate(arrs):\n",
    "        cv2.imshow(\"big_chunkimg\",arr)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        x = str(input(\"is lug: 1 not:0 ambigous hit enter\"))\n",
    "        \n",
    "            \n",
    "        if int(x) in [0,1]:\n",
    "            img_path = os.path.join(output_folder, f'{img_name}-{i}_{x}.png')\n",
    "            cv2.imwrite(img_path,arr)\n",
    "            \n",
    "        else:\n",
    "            small_arrs = np.array_split(img, img.shape[1]//5, axis=1)\n",
    "            for j, small_arr in enumerate(small_arrs):\n",
    "                cv2.imshow(\"small_img\",small_arr)\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()\n",
    "                x = str(input(\"is lug: 1 not:0 ambigous hit enter\"))\n",
    "\n",
    "                while int(cls) not in [0,1,2]:\n",
    "                    display.display(buttons_classification, clear=True)\n",
    "                    time.sleep(2)\n",
    "                img_path = os.path.join(output_folder,f'{img_name}-{i}-{j}_{x}.png')\n",
    "                cv2.imwrite(output_folder,small_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca6461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter1\n",
      "is lug: 1 not:0 ambigous hit enter1\n",
      "is lug: 1 not:0 ambigous hit enter1\n"
     ]
    }
   ],
   "source": [
    "annotate_img(img, img_path.split('/')[-1].split('.')[0], \"../data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee59770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfc9d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_images(img_paths, outpout_folder):\n",
    "    \n",
    "    for i in tqdm(range(len(img_paths))):\n",
    "        cnt = -1\n",
    "        path = img_paths[i]\n",
    "        arr = cv2.imread(path)\n",
    "        cv2.imshow(\"img\",arr)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        x = str(input(\"enter a char to proceed to annotating the img\"))\n",
    "        \n",
    "        if x:\n",
    "            img_name = img_path.split('/')[-1].split('.')[0]\n",
    "            annotate_img(img=arr, img_name=img_name, output_folder=output_folder)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e8816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                  | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a char to proceed to annotating the img4\n",
      "-1\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n",
      "is lug: 1 not:0 ambigous hit enter0\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"../data/raw/camA_Monochrome/\"\n",
    "output_folder = \"../data/annotated/cam_A_mono/\"\n",
    "\n",
    "img_paths = glob(f'{folder_path}/*.png')\n",
    "choose_images(img_paths, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "7aee4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def divide_image(img, new_width, img_name, folder):\n",
    "    small_arrs = np.array_split(img, img.shape[1]//new_width, axis=1)\n",
    "\n",
    "    \n",
    "    for i, ar in enumerate(small_arrs[2:]):\n",
    "        if ar.shape[-2] != new_width:\n",
    "            continue\n",
    "        im_p = os.path.join(folder, img_name+f'_{i}.png')\n",
    "        cv2.imwrite(im_p, ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "a09b146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                  | 0/452 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▎                                         | 4/452 [00:00<00:13, 33.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▋                                         | 8/452 [00:00<00:14, 31.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|█                                        | 12/452 [00:00<00:13, 31.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▍                                       | 16/452 [00:00<00:14, 31.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▊                                       | 20/452 [00:00<00:13, 31.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|██▏                                      | 24/452 [00:00<00:14, 30.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|██▌                                      | 28/452 [00:00<00:14, 29.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|██▊                                      | 31/452 [00:01<00:14, 29.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|███                                      | 34/452 [00:01<00:14, 28.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|███▎                                     | 37/452 [00:01<00:14, 28.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|███▋                                     | 40/452 [00:01<00:14, 28.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|███▉                                     | 43/452 [00:01<00:14, 28.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▏                                    | 46/452 [00:01<00:14, 28.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|████▌                                    | 50/452 [00:01<00:12, 31.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|████▉                                    | 54/452 [00:01<00:12, 31.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█████▎                                   | 58/452 [00:01<00:13, 29.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█████▌                                   | 62/452 [00:02<00:13, 29.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█████▉                                   | 66/452 [00:02<00:12, 29.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|██████▎                                  | 70/452 [00:02<00:12, 30.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████▋                                  | 74/452 [00:02<00:12, 29.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|███████                                  | 78/452 [00:02<00:12, 28.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|███████▍                                 | 82/452 [00:02<00:12, 30.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████▊                                 | 86/452 [00:02<00:12, 30.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|████████▏                                | 90/452 [00:03<00:12, 29.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|████████▍                                | 93/452 [00:03<00:12, 29.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|████████▋                                | 96/452 [00:03<00:12, 28.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|████████▊                               | 100/452 [00:03<00:12, 28.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|█████████                               | 103/452 [00:03<00:12, 28.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|█████████▍                              | 106/452 [00:03<00:12, 27.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|█████████▋                              | 110/452 [00:03<00:11, 28.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██████████                              | 114/452 [00:03<00:11, 30.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██████████▍                             | 118/452 [00:03<00:11, 29.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██████████▋                             | 121/452 [00:04<00:11, 29.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|███████████                             | 125/452 [00:04<00:10, 30.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|███████████▍                            | 129/452 [00:04<00:10, 30.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|███████████▊                            | 133/452 [00:04<00:10, 30.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|████████████                            | 137/452 [00:04<00:10, 30.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|████████████▍                           | 141/452 [00:04<00:10, 29.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|████████████▋                           | 144/452 [00:04<00:10, 29.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|█████████████                           | 147/452 [00:04<00:10, 29.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|█████████████▎                          | 150/452 [00:05<00:10, 29.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|█████████████▌                          | 153/452 [00:05<00:10, 29.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|█████████████▊                          | 156/452 [00:05<00:10, 29.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|██████████████▏                         | 160/452 [00:05<00:09, 30.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|██████████████▌                         | 164/452 [00:05<00:09, 29.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|██████████████▊                         | 167/452 [00:05<00:09, 29.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███████████████▏                        | 171/452 [00:05<00:09, 31.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███████████████▍                        | 175/452 [00:05<00:09, 30.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████▊                        | 179/452 [00:06<00:09, 29.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████████████████                        | 182/452 [00:06<00:09, 29.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████████████████▎                       | 185/452 [00:06<00:09, 27.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████████████████▋                       | 189/452 [00:06<00:09, 28.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████████████████▉                       | 192/452 [00:06<00:09, 28.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|█████████████████▎                      | 195/452 [00:06<00:08, 28.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|█████████████████▌                      | 198/452 [00:06<00:08, 28.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|█████████████████▊                      | 201/452 [00:06<00:08, 28.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|██████████████████                      | 204/452 [00:06<00:08, 29.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|██████████████████▎                     | 207/452 [00:06<00:08, 28.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|██████████████████▌                     | 210/452 [00:07<00:08, 29.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|██████████████████▉                     | 214/452 [00:07<00:07, 30.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|███████████████████▎                    | 218/452 [00:07<00:08, 28.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|███████████████████▌                    | 221/452 [00:07<00:08, 28.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|███████████████████▊                    | 224/452 [00:07<00:08, 28.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████████████████████                    | 227/452 [00:07<00:08, 27.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|████████████████████▎                   | 230/452 [00:07<00:08, 27.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|████████████████████▋                   | 234/452 [00:07<00:07, 28.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|████████████████████▉                   | 237/452 [00:08<00:07, 28.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████████████████████▏                  | 240/452 [00:08<00:07, 27.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████████████████████▌                  | 243/452 [00:08<00:07, 27.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████████████████████▊                  | 246/452 [00:08<00:07, 26.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|██████████████████████                  | 249/452 [00:08<00:07, 26.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|██████████████████████▎                 | 252/452 [00:08<00:07, 26.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|██████████████████████▌                 | 255/452 [00:08<00:07, 24.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|██████████████████████▉                 | 259/452 [00:08<00:07, 26.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|███████████████████████▏                | 262/452 [00:08<00:07, 26.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|███████████████████████▍                | 265/452 [00:09<00:07, 26.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|███████████████████████▋                | 268/452 [00:09<00:07, 24.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|███████████████████████▉                | 271/452 [00:09<00:06, 26.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|████████████████████████▏               | 274/452 [00:09<00:06, 26.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|████████████████████████▌               | 277/452 [00:09<00:06, 27.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|████████████████████████▊               | 280/452 [00:09<00:06, 28.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|█████████████████████████               | 283/452 [00:09<00:06, 27.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|█████████████████████████▎              | 286/452 [00:09<00:05, 28.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|█████████████████████████▌              | 289/452 [00:09<00:05, 27.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|█████████████████████████▊              | 292/452 [00:10<00:05, 27.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████████████████████████              | 295/452 [00:10<00:05, 27.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████████████████████████▍             | 299/452 [00:10<00:05, 28.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████████████████████████▊             | 303/452 [00:10<00:05, 29.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|███████████████████████████             | 306/452 [00:10<00:05, 28.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|███████████████████████████▎            | 309/452 [00:10<00:05, 28.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|███████████████████████████▌            | 312/452 [00:10<00:04, 28.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████████████████████████▉            | 315/452 [00:10<00:04, 28.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|████████████████████████████▏           | 318/452 [00:10<00:04, 28.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|████████████████████████████▍           | 321/452 [00:11<00:04, 28.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████▋           | 324/452 [00:11<00:04, 28.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████▉           | 327/452 [00:11<00:04, 25.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|█████████████████████████████▏          | 330/452 [00:11<00:04, 25.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|█████████████████████████████▍          | 333/452 [00:11<00:04, 25.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|█████████████████████████████▊          | 337/452 [00:11<00:04, 26.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|██████████████████████████████          | 340/452 [00:11<00:04, 27.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|██████████████████████████████▎         | 343/452 [00:11<00:04, 26.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|██████████████████████████████▌         | 346/452 [00:12<00:04, 26.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|██████████████████████████████▉         | 349/452 [00:12<00:04, 25.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████████████████████████████▏        | 352/452 [00:12<00:03, 26.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████████████████████████████▍        | 355/452 [00:12<00:03, 26.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████████████████████████████▋        | 358/452 [00:12<00:03, 26.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████████████████████████████▉        | 361/452 [00:12<00:03, 26.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████████████████████████████▏       | 364/452 [00:12<00:03, 22.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████████████████████████████▍       | 367/452 [00:12<00:03, 23.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████████████████████████████▋       | 370/452 [00:13<00:03, 24.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|█████████████████████████████████       | 373/452 [00:13<00:03, 25.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|█████████████████████████████████▎      | 376/452 [00:13<00:03, 24.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|█████████████████████████████████▌      | 379/452 [00:13<00:02, 24.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|█████████████████████████████████▊      | 382/452 [00:13<00:02, 23.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|██████████████████████████████████      | 385/452 [00:13<00:02, 24.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|██████████████████████████████████▎     | 388/452 [00:13<00:02, 24.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|██████████████████████████████████▌     | 391/452 [00:13<00:02, 25.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|██████████████████████████████████▉     | 395/452 [00:13<00:02, 28.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|███████████████████████████████████▏    | 398/452 [00:14<00:02, 26.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|███████████████████████████████████▍    | 401/452 [00:14<00:01, 27.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|███████████████████████████████████▊    | 404/452 [00:14<00:01, 27.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████████████████████████████████    | 407/452 [00:14<00:01, 28.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|████████████████████████████████████▎   | 411/452 [00:14<00:01, 29.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|████████████████████████████████████▋   | 414/452 [00:14<00:01, 29.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|████████████████████████████████████▉   | 417/452 [00:14<00:01, 24.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████████████████████████████████▎  | 421/452 [00:14<00:01, 26.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████████████████████████████████▌  | 424/452 [00:15<00:01, 26.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████████████████████████████████▊  | 427/452 [00:15<00:00, 26.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|██████████████████████████████████████  | 430/452 [00:15<00:00, 27.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|██████████████████████████████████████▍ | 434/452 [00:15<00:00, 28.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|██████████████████████████████████████▋ | 437/452 [00:15<00:00, 27.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|██████████████████████████████████████▉ | 440/452 [00:15<00:00, 27.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|███████████████████████████████████████▏| 443/452 [00:15<00:00, 27.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|███████████████████████████████████████▍| 446/452 [00:15<00:00, 27.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|███████████████████████████████████████▋| 449/452 [00:15<00:00, 26.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 452/452 [00:16<00:00, 28.06it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# folder_path = \"../data/annotated/B_lug/\"\n",
    "# output_folder = Path(\"../data/annotated/B/1/\")\n",
    "\n",
    "# img_paths = glob(f'{folder_path}/*.png')\n",
    "\n",
    "# for i in tqdm(range(len(img_paths))):\n",
    "#     p = img_paths[i]\n",
    "#     name =  p.split('\\\\')[-1].split('.')[0]\n",
    "#     img = cv2.imread(p)\n",
    "#     divide_image(img, 5, name, output_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "190490ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer, seed_everything, LightningDataModule\n",
    "from torch import nn\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "import torchmetrics\n",
    "import torch\n",
    "class Classifier(LightningModule):\n",
    "    \n",
    "    def __init__(self, numChannels = 1, classes = 1):\n",
    "        super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=numChannels, out_channels=4,\n",
    "#         kernel_size=(5, 5))\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.pool1 = nn.MaxPool2d((8, 1), stride=(8, 1))\n",
    "#         self.conv2 = nn.Conv2d(in_channels=4, out_channels=8,kernel_size=(1, 1))\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         # initialize first (and only) set of FC => RELU layers\n",
    "#         self.fc1 = nn.Linear(in_features=128, out_features=64)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         self.pool2 = nn.MaxPool2d((4, 1), stride=(4, 1))\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            #nn.Conv2d(kernel_size= (5,5), in_channels=1,out_channels=1),\n",
    "            nn.Linear(in_features=2680, out_features=32),\n",
    "            nn.ReLU(),\n",
    "#             nn.Linear(in_features=256, out_features= 64),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # initialize our softmax classifier\n",
    "#         self.fc2 = nn.Linear(in_features=32, out_features=1)\n",
    "        \n",
    "        self.train_acc = torchmetrics.Accuracy(task='binary')\n",
    "        self.val_acc = torchmetrics.Accuracy(task='binary')\n",
    "        self.test_acc = torchmetrics.Accuracy(task='binary')\n",
    "        \n",
    "    def configure_optimizers(self, learning_rate=1e-4):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "        \n",
    "    def forward(self,x):\n",
    "#         x = self.pool1(self.relu1(self.conv1(x)))\n",
    "#         x = self.pool2(self.relu2(self.conv2(x)))\n",
    "#         #x = torch.flatten(x,1)\n",
    "#         x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.flatten(x, start_dim=2)\n",
    "        \n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y = y.to(torch.float)\n",
    "        prediction = torch.flatten(self.forward(X)).to(torch.float)\n",
    "        loss = nn.BCELoss(reduction='none')(prediction, y)\n",
    "        self.log('train_loss', loss.mean())\n",
    "        \n",
    "        self.train_acc(prediction, y)\n",
    "        self.log('train_acc', self.train_acc, on_step=True, on_epoch=False)\n",
    "        return loss.mean()\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y = y.to(torch.float)\n",
    "        prediction = torch.flatten(self.forward(X)).to(torch.float)\n",
    "        loss = nn.BCELoss(reduction='none')(prediction, y)\n",
    "        self.log('test_loss', loss.mean())\n",
    "        \n",
    "        self.test_acc(prediction, y)\n",
    "        self.log('test_acc', self.test_acc, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss.mean()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y =  batch\n",
    "        y = y.to(torch.float)\n",
    "        prediction = torch.flatten(self.forward(X)).to(torch.float)\n",
    "        loss = nn.BCELoss(reduction='none')(prediction, y)\n",
    "        self.log('val_loss', loss.mean())\n",
    "        \n",
    "        self.val_acc(prediction, y)\n",
    "        self.log('val_acc', self.val_acc, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbc9a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class plData(LightningDataModule):\n",
    "    def __init__(self, train_path, test_path, val_path, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.val_path = val_path\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            torchvision.transforms.CenterCrop((536,5)),\n",
    "                            torchvision.transforms.Grayscale(),])\n",
    "       \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def setup(self, stage):\n",
    "        self.train_dataset = torchvision.datasets.ImageFolder(self.train_path, transform= self.transform)\n",
    "        self.val_dataset = torchvision.datasets.ImageFolder(self.val_path, transform= self.transform)\n",
    "        self.test_dataset = torchvision.datasets.ImageFolder(self.test_path, transform= self.transform)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size= self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "         return DataLoader(self.val_dataset, batch_size= self.batch_size, shuffle=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size= self.batch_size, shuffle=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c6eea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "batch_size = 128\n",
    "train_split = 0.8\n",
    "\n",
    "N_CONV_LAYERS = 1\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath='C:/work/freelance/wood-detection-segmentation/weights',\n",
    "    filename=\"Classifier_A-{epoch:02d}-{val_loss:.3f}\",\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        max_epochs= NUM_EPOCHS,\n",
    "        devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "        callbacks=[TQDMProgressBar(refresh_rate=20), checkpoint_callback],\n",
    "        logger=[TensorBoardLogger(\"logs/\", name=\"Classifier\"), CSVLogger(save_dir=\"logs/\")],\n",
    "    )\n",
    "\n",
    "\n",
    "model = Classifier()\n",
    "data = plData(train_path=\"../data/annotated/train/\", val_path= \"../data/annotated/val/\", test_path=\"../data/annotated/test\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "073430c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belja\\anaconda3\\envs\\wood\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:604: UserWarning: Checkpoint directory C:\\work\\freelance\\wood-detection-segmentation\\weights exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type           | Params\n",
      "---------------------------------------------\n",
      "0 | model     | Sequential     | 86.1 K\n",
      "1 | train_acc | BinaryAccuracy | 0     \n",
      "2 | val_acc   | BinaryAccuracy | 0     \n",
      "3 | test_acc  | BinaryAccuracy | 0     \n",
      "---------------------------------------------\n",
      "86.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "86.1 K    Total params\n",
      "0.344     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3742ba728cd34471907c76db8be0e393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belja\\anaconda3\\envs\\wood\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\belja\\anaconda3\\envs\\wood\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\belja\\anaconda3\\envs\\wood\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7ed085fd8949fa9c8dfbb3cee04e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e10e405b614b4bbcda986cc097cbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438efdb4e39547a9a360c33b3e157ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58412a2a430430ea584e6eb2dccc076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cab975bd93948b8aaed23847996c727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463f790fdb9c498e87b8e186c9bc49eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7588567f1743a190b4a02b53e82ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15245d82e828471ea12cd153fdd5ff61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e60fd0614a645e6bd42a5c1e9842397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b94c27272c41ab99d178868adc2469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66487ea2ec6b4b9db0571b2f080da0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441493b8bd3c48fa8200656fb8ed0fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f1f1e02e7f4d15ac43a3a76427dafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42215d3f039a493192ed7fde7447e79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2836c90852c743e993aa68acaf4a8150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266cb09ef1b844ab959ce91ed136f45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1668f169df39459485e311d21728c010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8ad9d721df46e2825ecf073ecc0570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05079077f8748f1bd050065dd0c7575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eacd4b7a059460abb1189c83ffacf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6352da28dd6b45c09cfa943da92210b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f482531",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "model= model.load_from_checkpoint(\"C:/work/freelance/wood-detection-segmentation/weights/Classifier_A-epoch=17-val_loss=0.004.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02f65008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\belja\\anaconda3\\envs\\wood\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f374c77ac1a14c96b5dc77ea66e0fc9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\r\n",
      "       Test metric             DataLoader 0\r\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\r\n",
      "     test_acc_epoch         0.9987804889678955\r\n",
      "        test_loss          0.006148072425276041\r\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "B_loader =  plData(train_path=\"../data/annotated/B/\", val_path= \"../data/annotated/B/\", test_path=\"../data/annotated/B/\")\n",
    "\n",
    "report = trainer.test(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e76bddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_loader.setup('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08026c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26597\n"
     ]
    }
   ],
   "source": [
    "print(len(B_loader.test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7742392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "path = '../data/annotated/1/img_100_35.png'\n",
    "\n",
    "img = torchvision.io.read_image(path).to(float)\n",
    "# img = torch.from_numpy(img).to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89568a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = torchvision.io.read_image('../data/annotated/1/img_100_35.png').to(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb6d1c40",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ToTensor.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tr \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: ToTensor.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "tr = torchvision.transforms.ToTensor(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1f1b91ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    \n",
    "    torchvision.transforms.Grayscale(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[170],\n",
    "        std=[90],\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa2cc5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff036911",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.save(\"wood_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "04264f0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [266]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mtest_dataloader:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "for x, y in data.test_dataloader:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6aeff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from PIL import Image\n",
    "#model.eval().to('cpu')\n",
    "total_duration = 0\n",
    "for i in range(1):\n",
    "    im = Image.open(\"../data/annotated/0/img_0_0.png\")\n",
    "    now = time.time()\n",
    "    img1 = B_loader.transform(im)\n",
    "    img1 = img1[None,:]\n",
    "    #rslt = model(img1)\n",
    "    total_duration += time.time()-now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42ab2493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 536, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "59e99a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it took 4.952708959579468 to make 1000 perdictions\n"
     ]
    }
   ],
   "source": [
    "print(f\"it took {total_duration} to make 1000 perdictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "f5f4faff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9948]]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6eee2f",
   "metadata": {},
   "source": [
    "#  Generate train set, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4940f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import multiprocessing\n",
    "import platform\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "images_0_path = \"../data/annotated/0/\"\n",
    "images_1_path = \"../data/annotated/1/\"\n",
    "\n",
    "imgs0 = glob(images_0_path + \"*.png\")\n",
    "imgs1 = glob(images_1_path + \"*.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0598511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7b35c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 76000/76000 [10:59<00:00, 115.25it/s]\n",
      "100%|███████████████████████████████████| 22395/22395 [03:11<00:00, 117.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "train_folder = \"../data/annotated/train/\"\n",
    "imgs0_train = imgs0[:int(0.8*len(imgs0))]\n",
    "imgs1_train = imgs1[:int(0.8*len(imgs1))]\n",
    "\n",
    "for i in tqdm(range(len(imgs0_train))):\n",
    "    img0 = imgs0[i]\n",
    "    shutil.copy(img0, train_folder+\"0/\")\n",
    "for i in tqdm(range(len(imgs1_train))):\n",
    "    img1 = imgs1[i]\n",
    "    shutil.copy(img1, train_folder+\"1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "21f6f84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 9500/9500 [00:21<00:00, 441.76it/s]\n",
      "100%|█████████████████████████████████████| 2799/2799 [00:04<00:00, 571.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# val data\n",
    "val = \"../data/annotated/val/\"\n",
    "imgs0_val = imgs0[int(0.8*len(imgs0)): int(0.9*len(imgs0))]\n",
    "imgs1_val = imgs1[int(0.8*len(imgs1)): int(0.9*len(imgs1))]\n",
    "\n",
    "for i in tqdm(range(len(imgs0_val))):\n",
    "    img0 = imgs0[i]\n",
    "    shutil.copy(img0, val+\"0/\")\n",
    "for i in tqdm(range(len(imgs1_val))):\n",
    "    img1 = imgs1[i]\n",
    "    shutil.copy(img1, val+\"1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6b15dfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 9500/9500 [00:16<00:00, 565.14it/s]\n",
      "100%|█████████████████████████████████████| 2800/2800 [00:05<00:00, 559.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "test = \"../data/annotated/test/\"\n",
    "imgs0_test = imgs0[int(0.9*len(imgs0)):]\n",
    "imgs1_test = imgs1[int(0.9*len(imgs1)):]\n",
    "\n",
    "for i in tqdm(range(len(imgs0_test))):\n",
    "    img0 = imgs0[i]\n",
    "    shutil.copy(img0, test+\"0/\")\n",
    "for i in tqdm(range(len(imgs1_test))):\n",
    "    img1 = imgs1[i]\n",
    "    shutil.copy(img1, test+\"1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5b734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wood",
   "language": "python",
   "name": "wood"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
