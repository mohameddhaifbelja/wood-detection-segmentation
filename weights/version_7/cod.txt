from pytorch_lightning import LightningModule, Trainer, seed_everything, LightningDataModule
from torch import nn
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.callbacks.progress import TQDMProgressBar
from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger
import torchmetrics

class Classifier(LightningModule):
    
    def __init__(self, numChannels = 1, classes = 1):
        super().__init__()
#         self.conv1 = nn.Conv2d(in_channels=numChannels, out_channels=4,
#         kernel_size=(5, 5))
#         self.relu1 = nn.ReLU()
#         self.pool1 = nn.MaxPool2d((8, 1), stride=(8, 1))
#         self.conv2 = nn.Conv2d(in_channels=4, out_channels=8,kernel_size=(1, 1))
#         self.relu2 = nn.ReLU()
#         # initialize first (and only) set of FC => RELU layers
#         self.fc1 = nn.Linear(in_features=128, out_features=64)
#         self.relu3 = nn.ReLU()
#         self.pool2 = nn.MaxPool2d((4, 1), stride=(4, 1))

        self.model = nn.Sequential(
            #nn.Conv2d(kernel_size= (5,5), in_channels=1,out_channels=1),
            nn.Linear(in_features=2680, out_features=64),
            nn.ReLU(),
#             nn.Linear(in_features=256, out_features= 64),
#             nn.ReLU(),
            nn.Linear(in_features=64, out_features=16),
            nn.ReLU(),
            nn.Linear(in_features=16, out_features=1),
            nn.Sigmoid()
        )

        # initialize our softmax classifier
#         self.fc2 = nn.Linear(in_features=32, out_features=1)
        
        self.train_acc = torchmetrics.Accuracy(task='binary')
        self.val_acc = torchmetrics.Accuracy(task='binary')
        self.test_acc = torchmetrics.Accuracy(task='binary')
        
    def configure_optimizers(self, learning_rate=1e-4):
        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)
        return optimizer
        
    def forward(self,x):
#         x = self.pool1(self.relu1(self.conv1(x)))
#         x = self.pool2(self.relu2(self.conv2(x)))
#         #x = torch.flatten(x,1)
#         x = torch.sigmoid(self.fc1(x))
        x = torch.flatten(x, start_dim=2)
        
        x = self.model(x)
        return x
    
    def training_step(self, batch, batch_idx):
        X, y = batch
        y = y.to(torch.float)
        prediction = torch.flatten(self.forward(X)).to(torch.float)
        loss = nn.BCELoss(reduction='none')(prediction, y)
        self.log('train_loss', loss.mean())
        
        self.train_acc(prediction, y)
        self.log('train_acc', self.train_acc, on_step=True, on_epoch=False)
        return loss.mean()
    
    
    def test_step(self, batch, batch_idx):
        X, y = batch
        y = y.to(torch.float)
        prediction = torch.flatten(self.forward(X)).to(torch.float)
        loss = nn.BCELoss(reduction='none')(prediction, y)
        self.log('test_loss', loss.mean())
        
        self.test_acc(prediction, y)
        self.log('test_acc', self.test_acc, on_step=True, on_epoch=True)
        
        return loss.mean()
    
    def validation_step(self, batch, batch_idx):
        X, y =  batch
        y = y.to(torch.float)
        prediction = torch.flatten(self.forward(X)).to(torch.float)
        loss = nn.BCELoss(reduction='none')(prediction, y)
        self.log('val_loss', loss.mean())
        
        self.val_acc(prediction, y)
        self.log('val_acc', self.val_acc, on_step=True, on_epoch=True)
        
        return loss.mean()